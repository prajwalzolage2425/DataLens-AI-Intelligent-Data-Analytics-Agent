{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìä DataLens AI - Intelligent Data Analytics Agent\n",
        "\n",
        "An autonomous AI-powered data analytics system that transforms raw datasets (CSV/Excel) into professional visualizations and interactive dashboards. Built with Google's Gemini API, this agent intelligently analyzes data, performs automated cleaning, and generates comprehensive visual insights.\n",
        "\n",
        "## üöÄ Workflow Pipeline\n",
        "**Setup Environment** ‚Üí **Initialize Gemini AI** ‚Üí **Load Data** ‚Üí **AI-Powered Analysis & Cleaning** ‚Üí **Generate Visualizations & Dashboard**\n",
        "\n",
        "## üéØ Core Capabilities\n",
        "- **AI-Driven Intelligence**: Leverages Gemini API for automated data quality assessment and insights generation\n",
        "- **Interactive Data Upload**: Seamless file upload widget supporting CSV and Excel formats\n",
        "- **Automated Cleaning**: Generates and applies intelligent cleaning code based on data profiling\n",
        "- **Smart Outlier Handling**: Uses statistical capping methods to preserve data integrity\n",
        "- **Production-Ready Output**: Delivers ML-ready datasets with proper encoding and standardization\n",
        "- **Professional Visualizations**: Creates publication-quality charts and interactive dashboards"
      ],
      "metadata": {
        "id": "SmB3GCxZXXW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1 : Environment Setup\n",
        "Installing all required dependencies for the data analytics agent including data processing libraries (pandas, numpy), visualization tools (matplotlib, seaborn, plotly), machine learning utilities (scikit-learn), and Google Generative AI SDK for agent capabilities."
      ],
      "metadata": {
        "id": "zyZV9-CXWAMK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-D1LvXEGVfC"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip --quiet  # Update pip to latest version\n",
        "!pip install pandas numpy matplotlib seaborn plotly scikit-learn ipywidgets jsonschema \\\n",
        "            google-generativeai google-auth google-auth-oauthlib --quiet  # Data processing, visualization, and Gemini API libraries\n",
        "!pip install openpyxl xlrd jupyterlab --quiet  # Excel file handling libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2: Import Libraries and Verify Installation\n",
        "Importing all essential libraries and verifying successful installation. This cell performs safe imports with error handling, checks for missing packages, displays library versions, and configures visualization settings for the notebook environment."
      ],
      "metadata": {
        "id": "GsAV-1BBYS_0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FJ9P_wUIu1h"
      },
      "outputs": [],
      "source": [
        "# Import cell\n",
        "\n",
        "import importlib\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "# List of required packages for verification\n",
        "expected = [\n",
        "    \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"plotly\",\n",
        "    \"google.genai\", \"ipywidgets\", \"jsonschema\", \"sklearn\", \"tqdm\"\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for pkg in expected:\n",
        "    try:\n",
        "        importlib.import_module(pkg)  # Check if package is importable\n",
        "    except Exception as exc:\n",
        "        missing.append((pkg, str(exc)))\n",
        "\n",
        "if missing:\n",
        "    print(\"‚ö†Ô∏è Some packages are not importable. Please run the install cell, restart the runtime, and run this cell again.\\n\")\n",
        "    for pkg, err in missing:\n",
        "        print(f\"- {pkg}: {err}\")\n",
        "    # Stop here to avoid confusing errors\n",
        "else:\n",
        "    # Import all libraries with proper aliases\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "        import matplotlib\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Enable inline plotting for IPython/Colab environments\n",
        "        try:\n",
        "            ip = get_ipython()\n",
        "            ip.run_line_magic(\"matplotlib\", \"inline\")\n",
        "        except NameError:\n",
        "            pass  # Not in IPython environment\n",
        "\n",
        "        import seaborn as sns\n",
        "        import plotly\n",
        "        import plotly.express as px\n",
        "        import plotly.io as pio\n",
        "\n",
        "        # Set Plotly renderer for Colab compatibility\n",
        "        try:\n",
        "            if \"colab\" in pio.renderers:\n",
        "                pio.renderers.default = \"colab\"\n",
        "            else:\n",
        "                pio.renderers.default = pio.renderers.default\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        import google.genai as genai\n",
        "        import ipywidgets as widgets\n",
        "        import jsonschema as _jsonschema\n",
        "        from jsonschema import validate, Draft7Validator\n",
        "        import sklearn\n",
        "        from tqdm import tqdm\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Import error after initial checks:\", e)\n",
        "        print(\"Tip: re-run the install cell, restart the runtime, then run this cell again.\")\n",
        "        raise\n",
        "\n",
        "    # Display version information for debugging\n",
        "    def safe_ver(m):\n",
        "        return getattr(m, \"__version__\", getattr(m, \"version\", \"unknown\"))\n",
        "\n",
        "    print(\"\\nLibrary versions:\")\n",
        "    print(\"pandas        :\", safe_ver(pd))\n",
        "    print(\"numpy         :\", safe_ver(np))\n",
        "    print(\"matplotlib    :\", safe_ver(matplotlib))\n",
        "    print(\"seaborn       :\", safe_ver(sns))\n",
        "    print(\"plotly        :\", safe_ver(plotly))\n",
        "    print(\"google.genai  :\", getattr(genai, \"__version__\", \"unknown\"))\n",
        "    print(\"ipywidgets    :\", safe_ver(widgets))\n",
        "    print(\"jsonschema    :\", safe_ver(_jsonschema))\n",
        "    print(\"scikit-learn  :\", safe_ver(sklearn))\n",
        "    print(\"tqdm          :\", safe_ver(tqdm))\n",
        "\n",
        "    # Configure default visualization settings\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    warnings.filterwarnings(\"ignore\")  # Suppress non-critical warnings\n",
        "    print(\"\\n‚úÖ Imports complete ‚Äî ready for the next step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3: API Key Configuration\n",
        "Securely retrieving the Gemini API key from Google Colab's userdata storage. This ensures credentials are not hardcoded in the notebook for security best practices."
      ],
      "metadata": {
        "id": "ev4pFRuDYv33"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovHwPzyKI-vK"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get(\"GEMINI_API_KEY\")  # Fetch API key from Colab secrets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 4: Initialize Gemini AI Client\n",
        "Setting up the Gemini AI client using the secure API authentication. This creates a reusable client instance for all subsequent AI-powered data analysis operations."
      ],
      "metadata": {
        "id": "KmhYI3gyY467"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdyFGLlUJDC2"
      },
      "outputs": [],
      "source": [
        "# Step 2 ‚Äî Correct Gemini API Setup (google-genai v1.49+)\n",
        "\n",
        "import google.genai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "def init_gemini():\n",
        "    \"\"\"\n",
        "    Initialize Gemini client with API key from Colab secrets.\n",
        "    Returns configured client instance for AI operations.\n",
        "    \"\"\"\n",
        "    api_key = userdata.get(\"GEMINI_API_KEY\")  # Retrieve API key securely\n",
        "    if not api_key:\n",
        "        raise ValueError(\"‚ùå Gemini API key not found in Colab Secrets!\")\n",
        "\n",
        "    client = genai.Client(api_key=api_key)  # Create Gemini client instance\n",
        "    print(\"‚úÖ Gemini Client created (secure).\")\n",
        "\n",
        "    return client\n",
        "\n",
        "# Initialize client for global use\n",
        "client = init_gemini()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 5: Data Upload and Automated Summary Generation\n",
        "Interactive file upload interface that accepts CSV/Excel files and automatically generates a comprehensive dataset summary. This summary includes statistical metrics, missing value analysis, and data type information for AI-powered processing."
      ],
      "metadata": {
        "id": "nWSPlTxRZRM-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghNL0Xf0kUPS"
      },
      "outputs": [],
      "source": [
        "# Step 3 ‚Äî Upload Dataset + Auto Summary\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "def upload_dataset():\n",
        "    \"\"\"\n",
        "    Interactive file upload widget for CSV/Excel files.\n",
        "    Returns loaded DataFrame with preview and basic info.\n",
        "    \"\"\"\n",
        "    print(\"üìÅ Upload your CSV or Excel file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    file_name = next(iter(uploaded))\n",
        "\n",
        "    # Load dataset based on file extension\n",
        "    if file_name.lower().endswith(\".csv\"):\n",
        "        df = pd.read_csv(file_name)\n",
        "    elif file_name.lower().endswith((\".xlsx\", \".xls\")):\n",
        "        df = pd.read_excel(file_name)\n",
        "    else:\n",
        "        raise ValueError(\"‚ùå Unsupported file format. Upload CSV or Excel.\")\n",
        "\n",
        "    print(\"\\n‚úÖ File Loaded Successfully!\\n\")\n",
        "\n",
        "    # Display preview and basic statistics\n",
        "    display(df.head())\n",
        "    print(\"\\nüìè Shape:\", df.shape)\n",
        "    print(\"\\nüìä Data Types:\\n\", df.dtypes)\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------- Summary Generator -------- #\n",
        "\n",
        "def generate_dataset_summary(df):\n",
        "    \"\"\"\n",
        "    Generate comprehensive column-wise summary for AI analysis.\n",
        "    Includes stats, missing values, and data type information.\n",
        "    \"\"\"\n",
        "    summary = {}\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_data = df[col]\n",
        "        info = {\n",
        "            \"dtype\": str(col_data.dtype),\n",
        "            \"missing_count\": int(col_data.isna().sum()),\n",
        "            \"missing_percent\": float((col_data.isna().mean() * 100)),\n",
        "            \"unique_count\": int(col_data.nunique())\n",
        "        }\n",
        "\n",
        "        # Calculate numeric statistics\n",
        "        if pd.api.types.is_numeric_dtype(col_data):\n",
        "            info[\"min\"] = float(col_data.min())\n",
        "            info[\"max\"] = float(col_data.max())\n",
        "            info[\"mean\"] = float(col_data.mean())\n",
        "            info[\"std\"] = float(col_data.std())\n",
        "\n",
        "        # Extract categorical samples\n",
        "        if pd.api.types.is_object_dtype(col_data) or pd.api.types.is_categorical_dtype(col_data):\n",
        "            info[\"unique_values\"] = col_data.dropna().unique().tolist()[:10]  # Top 10 samples\n",
        "\n",
        "        summary[col] = info\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "# -------- Run Upload + Summary -------- #\n",
        "\n",
        "df = upload_dataset()  # Load dataset interactively\n",
        "dataset_summary = generate_dataset_summary(df)  # Generate AI-ready summary\n",
        "\n",
        "print(\"\\nüìò Dataset Summary for LLM:\")\n",
        "dataset_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 6: AI-Powered Data Cleaning Agent\n",
        "Implements an intelligent cleaning agent that analyzes dataset summary and generates customized cleaning recommendations with executable Python code. Uses Gemini AI to identify data quality issues and propose automated solutions."
      ],
      "metadata": {
        "id": "56bIz_ROZdh0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htn9U9yMkXUk"
      },
      "outputs": [],
      "source": [
        "# Step 4 ‚Äî Simple Cleaning Agent (NO JSON)\n",
        "\n",
        "def build_cleaning_prompt(summary):\n",
        "    \"\"\"\n",
        "    Construct prompt for Gemini to analyze data quality issues.\n",
        "    Returns structured prompt requesting problems, recommendations, and code.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\n",
        "You are a professional data cleaning agent.\n",
        "\n",
        "Analyze the dataset summary below and produce:\n",
        "\n",
        "1. A list of problems found (missing values, negative values, inconsistent categories, invalid ranges, dtype issues, outliers).\n",
        "2. Recommended cleaning steps.\n",
        "3. Python code that cleans the dataframe (df) and outputs df_clean.\n",
        "\n",
        "Notes:\n",
        "- Do NOT use JSON.\n",
        "- Do NOT use markdown.\n",
        "- Only plain text and Python code.\n",
        "- Python code must assume the dataframe is already named df.\n",
        "- Cleaning code must output df_clean.\n",
        "\n",
        "Dataset Summary:\n",
        "{summary}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def ask_gemini_cleaning(prompt):\n",
        "    \"\"\"\n",
        "    Send cleaning analysis prompt to Gemini API.\n",
        "    Returns AI-generated cleaning recommendations and code.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=\"gemini-2.5-pro\",  # Use latest Gemini model\n",
        "            contents=prompt,\n",
        "            config={\"response_mime_type\": \"text/plain\"}  # Plain text response\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 7: Execute AI Data Cleaning Analysis\n",
        "Generates and sends the cleaning prompt to Gemini AI, receiving intelligent analysis of data quality issues along with automated cleaning recommendations and executable code."
      ],
      "metadata": {
        "id": "DeQvuZ82Zr9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4AQfvkUkfT2"
      },
      "outputs": [],
      "source": [
        "cleaning_prompt = build_cleaning_prompt(dataset_summary)  # Create cleaning prompt with dataset summary\n",
        "cleaning_output = ask_gemini_cleaning(cleaning_prompt)  # Get AI-generated cleaning recommendations\n",
        "\n",
        "print(cleaning_output)  # Display problems, recommendations, and cleaning code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 8: Intelligent Outlier Capping\n",
        "Applies statistical outlier capping to numerical columns using the 99th percentile method. This preserves data points while reducing extreme values, ensuring dataset integrity for analysis and modeling."
      ],
      "metadata": {
        "id": "8pY5iv_tahrs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8XBjEdotTJe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a copy of the dataframe to work on\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Dynamically identify numerical columns for outlier capping\n",
        "# Exclude identifier columns (like 'Store') and binary flags (like 'Holiday_Flag')\n",
        "numeric_cols_for_capping = [\n",
        "    col for col in df_clean.select_dtypes(include=np.number).columns\n",
        "    if col not in ['Store', 'Holiday_Flag']  # Adjust exclusions based on dataset\n",
        "]\n",
        "\n",
        "print(f\"Applying 99th percentile capping to columns: {numeric_cols_for_capping}\")\n",
        "\n",
        "# Apply 99th percentile capping to reduce outlier impact\n",
        "for col in numeric_cols_for_capping:\n",
        "    upper_cap = df_clean[col].quantile(0.99)  # Calculate 99th percentile threshold\n",
        "    df_clean[col] = df_clean[col].clip(upper=upper_cap)  # Cap values at threshold\n",
        "    print(f\"  - Capped '{col}' at: {upper_cap:.2f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Outlier capping applied to selected numerical columns.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 9: Comprehensive Data Preprocessing Pipeline\n",
        "Complete data preprocessing including missing value detection, negative value handling, column normalization, categorical encoding, feature standardization, and outlier visualization. Prepares ML-ready dataset with proper scaling and encoding."
      ],
      "metadata": {
        "id": "PmTkM4oHbBJW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq70WxmpteSl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Detect Missing Values\n",
        "# -----------------------------\n",
        "print(\"Missing Values:\")\n",
        "print(df.isna().sum())  # Display missing value counts per column\n",
        "\n",
        "# Create working copy for cleaning\n",
        "df_clean = df.copy()\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Detect Negative Values\n",
        "# -----------------------------\n",
        "numeric_cols_before_norm = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "negatives = {}\n",
        "for col in numeric_cols_before_norm:\n",
        "    negatives[col] = df_clean[df_clean[col] < 0].shape[0]  # Count negative values\n",
        "\n",
        "print(\"\\nNegative Values Per Column:\")\n",
        "print(negatives)\n",
        "\n",
        "# Convert negative values to absolute (safe default approach)\n",
        "for col in numeric_cols_before_norm:\n",
        "    df_clean[col] = df_clean[col].abs()\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Normalize Column Names\n",
        "# -----------------------------\n",
        "df_clean.columns = (\n",
        "    df_clean.columns\n",
        "    .str.strip()          # Remove whitespace\n",
        "    .str.lower()          # Convert to lowercase\n",
        "    .str.replace(' ', '_') # Replace spaces with underscores\n",
        ")\n",
        "\n",
        "print(\"\\nNormalized Columns:\")\n",
        "print(df_clean.columns.tolist())\n",
        "\n",
        "# Re-identify numeric columns after normalization\n",
        "numeric_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Detect and Encode Categorical Columns\n",
        "# -----------------------------\n",
        "categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
        "print(\"\\nCategorical Columns:\", categorical_cols)\n",
        "\n",
        "df_encoded = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=True)  # One-hot encoding\n",
        "\n",
        "print(\"\\nEncoded DataFrame Shape:\", df_encoded.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Standardize Numerical Features\n",
        "# -----------------------------\n",
        "scaler = StandardScaler()  # Initialize scaler for z-score normalization\n",
        "\n",
        "num_cols = df_encoded.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "df_encoded[num_cols] = scaler.fit_transform(df_encoded[num_cols])  # Apply standardization\n",
        "\n",
        "print(\"\\nNumerical Columns Standardized\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Outlier Visualizations\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df_clean[numeric_cols])  # Boxplot to visualize outliers\n",
        "plt.title(\"Boxplot of Numerical Columns (Before Scaling)\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "df_clean[numeric_cols].hist(bins=20, figsize=(12, 6))  # Distribution histograms\n",
        "plt.suptitle(\"Distribution of Numerical Columns (Before Scaling)\")\n",
        "plt.show()\n",
        "\n",
        "# Final cleaned dataset ready for ML\n",
        "df_final = df_encoded.copy()\n",
        "\n",
        "print(\"\\nCLEANING COMPLETE!\")\n",
        "print(\"Final Shape:\", df_final.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 10: AI-Powered Visualization Code Generation\n",
        "Generates Python code for 10 professional visualizations using Gemini AI. Creates diverse chart types (histograms, bar charts, scatter plots, heatmaps, etc.) tailored to the cleaned dataset structure."
      ],
      "metadata": {
        "id": "lOVpdPZ1bMgD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0w88REhuorp"
      },
      "outputs": [],
      "source": [
        "viz_prompt = f\"\"\"\n",
        "You are a Visualization Agent.\n",
        "\n",
        "Generate Python code that creates exactly **10 visualizations** using df_clean:\n",
        "\n",
        "1. Histogram\n",
        "2. Bar chart\n",
        "3. Line chart (trend)\n",
        "4. Scatter plot\n",
        "5. Box plot\n",
        "6. Heatmap (pivot)\n",
        "7. Pie chart\n",
        "8. Correlation heatmap\n",
        "9. Category distribution bar chart\n",
        "10. Daily trend line chart\n",
        "\n",
        "Rules:\n",
        "- Use matplotlib + seaborn only.\n",
        "- Do NOT use plotly.\n",
        "- Do NOT output markdown.\n",
        "- Do NOT wrap in backticks.\n",
        "- Output only executable Python code.\n",
        "\n",
        "Dataset columns: {list(df_clean.columns)}\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"models/gemini-2.5-flash\",  # Use fast model for code generation\n",
        "    contents=[viz_prompt]\n",
        ")\n",
        "\n",
        "print(response.text)  # Display generated visualization code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 11: Gemini API Helper Function\n",
        "Creates a reusable function to streamline Gemini API calls. Simplifies prompt submission and response retrieval for various AI-powered tasks throughout the notebook."
      ],
      "metadata": {
        "id": "qdQFX9lkbZTv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ntrrbGouqr6"
      },
      "outputs": [],
      "source": [
        "def prompt_gemini(prompt):\n",
        "    \"\"\"\n",
        "    Send prompt to Gemini API and return response.\n",
        "    Simplifies API calls for text generation tasks.\n",
        "    \"\"\"\n",
        "    response = client.models.generate_content(\n",
        "        model=\"models/gemini-2.5-flash\",  # Use efficient flash model\n",
        "        contents=[prompt]\n",
        "    )\n",
        "    return response.text  # Return AI-generated response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 12: Generate Visualization Code\n",
        "Executes the visualization prompt through Gemini API to generate complete Python code for all 10 chart types. Displays the AI-generated code ready for execution."
      ],
      "metadata": {
        "id": "DEcp3lsObkzD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9orUmF7veYj"
      },
      "outputs": [],
      "source": [
        "viz_code = prompt_gemini(viz_prompt)  # Generate visualization code using helper function\n",
        "\n",
        "print(viz_code)  # Display generated code for review before execution"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 13: Execute AI-Generated Visualizations\n",
        "Executes the AI-generated visualization code dynamically to render all 10 charts. Uses Python's exec() to run the code and display professional visualizations of the cleaned dataset."
      ],
      "metadata": {
        "id": "BXIL-mEXbuBA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oMVlvSfvfwO"
      },
      "outputs": [],
      "source": [
        "# The 'viz_prompt' variable is already defined from a previous cell (z0w88REhuorp).\n",
        "# The line 'viz_prompt = build_visualization_prompt(dataset_summary)' caused a NameError as the function was not defined.\n",
        "# We will use the existing 'viz_prompt' and generate the visualization code.\n",
        "viz_code = prompt_gemini(viz_prompt)  # Generate visualization code\n",
        "\n",
        "# Execute the generated code directly to render charts\n",
        "exec(viz_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 14: Preview and Execute Visualization Code\n",
        "Displays a preview of the generated visualization code (first 300 characters) for verification, then executes the complete code to render all visualizations in the notebook."
      ],
      "metadata": {
        "id": "U3ksfGiub4WW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C26YZQLJx3CN"
      },
      "outputs": [],
      "source": [
        "print(viz_code[:300])  # Preview first 300 characters of generated code\n",
        "exec(viz_code)  # Execute complete visualization code to display charts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 15: Generate Interactive Dashboard Prompt\n",
        "Constructs a comprehensive prompt for Gemini AI to create a fully interactive Plotly dashboard with KPI cards, dynamic filters, and 4 combined visualizations that auto-update based on user selections."
      ],
      "metadata": {
        "id": "97f8r0C7ck2U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fvP0XWMx6g9"
      },
      "outputs": [],
      "source": [
        "dash_prompt = f\"\"\"\n",
        "You are a Python data analyst agent.\n",
        "\n",
        "Goal: Generate a complete INTERACTIVE DASHBOARD using:\n",
        "- plotly.express\n",
        "- plotly.graph_objects\n",
        "- ipywidgets\n",
        "- pandas\n",
        "\n",
        "The dashboard must work inside Jupyter/Colab.\n",
        "\n",
        "IMPORTANT RULES:\n",
        "- Output ONLY Python code (no explanation, no markdown)\n",
        "- Do NOT use JSON responses\n",
        "- Do NOT invent new columns; use only columns present in the dataset summary\n",
        "- Assume df_clean is already loaded and cleaned. **DO NOT create a dummy df_clean DataFrame.**\n",
        "- Use the provided df_clean columns: {list(df_clean.columns)}.\n",
        "- **IMPORTANT: Always use the EXACT normalized column names from 'df_clean' for all operations, which are: {list(df_clean.columns)}.**\n",
        "- **CRITICAL: All HTML elements (e.g., titles, KPI card content) placed inside `ipywidgets` containers (`VBox`, `HBox`) MUST be created using `ipywidgets.HTML` (e.g., `widgets.HTML('<h3>My Title</h3>')`) for full compatibility. Do NOT use `IPython.display.HTML`.**\n",
        "- Create a fully interactive dashboard\n",
        "- The dashboard should have a **clean layout** with 4 combined charts and filter widgets\n",
        "- Dashboard must **auto-update all charts** when filters change\n",
        "- Charts should be colorful, visually engaging, and easy to interpret\n",
        "- Use maps for country-level sales, vibrant colors for bars, lines, and scatter plots\n",
        "- Include hover tooltips with detailed information\n",
        "\n",
        "DATASET SUMMARY:\n",
        "{dataset_summary}  # Pass dataset structure to AI\n",
        "\n",
        "REQUIREMENTS:\n",
        "\n",
        "1Ô∏è‚É£ KPI CARDS: Total Sales, Total Quantity, Average Order Value, Total Products Sold\n",
        "2Ô∏è‚É£ Interactive Filters:\n",
        "   - Country (multi-select)\n",
        "   - Product (multi-select)\n",
        "   - Date Range (slider or date picker)\n",
        "3Ô∏è‚É£ 4 Combined Charts:\n",
        "   a) Sales Over Time (Line Chart with different colors per product or region)\n",
        "   b) Sales by Product (Colorful Bar Chart with hover info)\n",
        "   c) Sales by Country (Interactive Map or Pie/Bar Chart with vibrant colors)\n",
        "   d) Sales vs Quantity (Scatter Plot with color representing Country or Product)\n",
        "4Ô∏è‚É£ Charts must respond to filter changes automatically\n",
        "5Ô∏è‚É£ Clean layout: KPIs on top, filters on left/top, charts in a 2x2 grid\n",
        "6Ô∏è‚É£ All charts should have titles, axis labels, and legends\n",
        "\n",
        "\n",
        "Return ONLY Python code for this interactive, colorful dashboard.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 16: Enable Custom Widget Manager\n",
        "Activates Google Colab's custom widget manager to enable proper rendering of interactive ipywidgets components required for the dashboard interface."
      ],
      "metadata": {
        "id": "fGsaIAL_cxNs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb146182"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()  # Enable ipywidgets support in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 17: Generate and Execute Dashboard Code\n",
        "\n",
        "Uses Gemini API to create dashboard visualization code, then executes it dynamically."
      ],
      "metadata": {
        "id": "FP2SbHGMcwvO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iim5zYELz4RG"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import re\n",
        "\n",
        "# Request dashboard code generation from Gemini API\n",
        "dashboard_code = prompt_gemini(dash_prompt)\n",
        "\n",
        "# Strip markdown code block formatting if present\n",
        "if dashboard_code.startswith('```python') and dashboard_code.endswith('```'):\n",
        "    dashboard_code = dashboard_code[len('```python'):-len('```')].strip()\n",
        "\n",
        "# Execute the generated dashboard code\n",
        "try:\n",
        "    exec(dashboard_code)\n",
        "except Exception as e:\n",
        "    print(f\"Error executing dashboard code: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 18: Auto-Generated Predictive Data Analysis Report\n",
        "\n",
        "Automatically detects dataset structure (numeric, categorical, datetime columns) and generates a comprehensive predictive analysis report including data quality metrics, insights, and recommendations."
      ],
      "metadata": {
        "id": "YAsPyZ6jedI5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I8DF7D7OM2g"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "#===============================#\n",
        "#   AUTO-DETECT DATASET INFO    #\n",
        "#===============================#\n",
        "\n",
        "df = df_clean.copy()\n",
        "\n",
        "# Identify column types for automated analysis\n",
        "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "datetime_cols = df.select_dtypes(include=[\"datetime64\"]).columns.tolist()\n",
        "\n",
        "# Select primary columns for analysis if available\n",
        "main_metric = numeric_cols[0] if numeric_cols else None\n",
        "second_metric = numeric_cols[1] if len(numeric_cols) > 1 else None\n",
        "main_category = categorical_cols[0] if categorical_cols else None\n",
        "\n",
        "#===============================#\n",
        "#      BUILD AUTO REPORT        #\n",
        "#===============================#\n",
        "\n",
        "prompt = \"\"\"\n",
        "You are a data analyst AI. Generate a Predictive report in Plain Text format.\n",
        "DO NOT assume any specific dataset structure.\n",
        "Automatically analyze the dataset provided (df_clean).\n",
        "Include: Title, Executive Summary, Dataset Profile, Data Quality Report,\n",
        "Insights, Visual Summary, ML Results, Recommendations, Conclusion.\n",
        "Use generic logic compatible with ANY dataset.\n",
        "\"\"\"\n",
        "\n",
        "# Build dynamic report with dataset statistics\n",
        "predictive_report_text = f\"\"\"\n",
        "# üìù Predictive Data Analysis Report\n",
        "\n",
        "---\n",
        "\n",
        "## Executive Summary\n",
        "This report presents a **predictive analytical summary** of the uploaded dataset.\n",
        "The analysis covers data quality, patterns, trends, and estimated future behavior\n",
        "based on detected numerical and categorical variables.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Profile\n",
        "- **Total Rows:** `{len(df)}`\n",
        "- **Total Columns:** `{df.shape[1]}`\n",
        "- **Numeric Columns:** `{numeric_cols}`\n",
        "- **Categorical Columns:** `{categorical_cols}`\n",
        "- **Datetime Columns:** `{datetime_cols}`\n",
        "\n",
        "---\n",
        "\n",
        "## Data Quality Report\n",
        "- **Missing Values:** `{df.isnull().sum().to_dict()}`\n",
        "- **Duplicate Rows:** `{df.duplicated().sum()}`\n",
        "- **Memory Usage (KB):** `{df.memory_usage().sum() / 1024:.2f}`\n",
        "\n",
        "---\n",
        "\n",
        "## Insights\n",
        "\"\"\"\n",
        "\n",
        "# Generate categorical insight if available\n",
        "if main_category:\n",
        "    top_cat = df[main_category].value_counts().idxmax()\n",
        "    predictive_report_text += f\"- **Most frequent {main_category}:** `{top_cat}`\\n\"\n",
        "else:\n",
        "    predictive_report_text += \"- No categorical insights available.\\n\"\n",
        "\n",
        "# Generate numeric insight if available\n",
        "if main_metric:\n",
        "    avg_metric = df[main_metric].mean()\n",
        "    predictive_report_text += f\"- **Average {main_metric}:** `{avg_metric:,.2f}`\\n\"\n",
        "else:\n",
        "    predictive_report_text += \"- No numeric insights available.\\n\"\n",
        "\n",
        "predictive_report_text += \"\"\"\n",
        "\n",
        "---\n",
        "\n",
        "## Visual Summary\n",
        "- Dataset contains patterns in numerical and categorical variables.\n",
        "- Trends and distributions indicate potential seasonality, clusters, or outliers.\n",
        "*(Visuals not included in this text version)*\n",
        "\n",
        "---\n",
        "\n",
        "## ML Results\n",
        "\"\"\"\n",
        "\n",
        "# Generate simple prediction if numeric column exists\n",
        "if main_metric:\n",
        "    forecast = df[main_metric].mean() * 1.1\n",
        "    predictive_report_text += f\"- Predicted value for next period of `{main_metric}`: `{forecast:,.2f}` (‚âà10% growth assumption)\\n\"\n",
        "else:\n",
        "    predictive_report_text += \"- No numeric target available for prediction.\\n\"\n",
        "\n",
        "predictive_report_text += \"\"\"\n",
        "\n",
        "---\n",
        "\n",
        "## Recommendations\n",
        "1. Improve data quality by addressing missing and duplicate values.\n",
        "2. Investigate factors influencing key numeric or categorical trends.\n",
        "3. Consider applying forecasting or clustering models for deeper insights.\n",
        "4. Use detected patterns to optimize strategy and planning.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "This dataset reveals **actionable patterns** based on statistical and predictive analysis.\n",
        "Further modeling can refine predictions and uncover deeper relationships among variables.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Render the report as formatted markdown\n",
        "display(Markdown(predictive_report_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 19: Business Intelligence Recommendations Report\n",
        "\n",
        "Generates a structured business intelligence report with actionable recommendations, optimization strategies, risk signals, opportunities, and next steps based on dashboard analysis."
      ],
      "metadata": {
        "id": "MBa3dIltepuT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWEdlBKYPxQK"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Prepare prompt for business intelligence analysis\n",
        "prompt = \"\"\"\n",
        "You are a Business Intelligence AI. Analyze the dataset from the interactive dashboard\n",
        "(sales, quantity, products, countries, dates) and provide a structured actionable output.\n",
        "Include the following sections:\n",
        "\n",
        "1. Business Recommendations\n",
        "2. Optimization Strategies\n",
        "3. Risk Signals\n",
        "4. Opportunities\n",
        "5. Next Steps for Deeper Analysis\n",
        "\n",
        "Provide professional, concise, and clear insights that can guide business decisions.\n",
        "\"\"\"\n",
        "\n",
        "# Generate business recommendations report\n",
        "# Note: Replace with actual Gemini API call for dynamic AI-generated insights\n",
        "recommendations_text = f\"\"\"\n",
        "# üìä Business Recommendations & Strategic Insights\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ Business Recommendations\n",
        "- Focus on **high-demand products** identified from historical sales trends.\n",
        "- Allocate marketing budget to **top-performing regions** for maximum ROI.\n",
        "- Streamline inventory management to **reduce overstock and stockouts**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ Optimization Strategies\n",
        "- Implement **dynamic pricing** for products with seasonal fluctuations.\n",
        "- Optimize delivery routes for **faster shipping and cost savings**.\n",
        "- Automate reporting dashboards to track **real-time sales performance**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ Risk Signals\n",
        "- Products with **declining sales trend** need marketing attention.\n",
        "- Regions with **low sales growth** may indicate market saturation.\n",
        "- Monitor inventory discrepancies to **prevent financial losses**.\n",
        "\n",
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ Opportunities\n",
        "- Upsell complementary products in high-demand categories.\n",
        "- Expand to **emerging markets** identified from regional sales data.\n",
        "- Launch targeted promotions for underperforming products with potential.\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ Next Steps for Deeper Analysis\n",
        "- Conduct **predictive modeling** for sales forecasting by product and region.\n",
        "- Perform **customer segmentation** to tailor marketing campaigns.\n",
        "- Analyze **seasonal and trend patterns** to optimize stock and pricing strategies.\n",
        "\"\"\"\n",
        "\n",
        "# Render the report as formatted markdown\n",
        "display(Markdown(recommendations_text))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}